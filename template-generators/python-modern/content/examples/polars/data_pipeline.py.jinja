"""
Polars Data Processing Pipeline Example
Demonstrates modern data processing patterns with Polars.
"""
import polars as pl
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Union
from datetime import datetime, timedelta
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataPipeline:
    """Example data processing pipeline using Polars"""
    
    def __init__(self, output_dir: Path = None):
        self.output_dir = output_dir or Path("data/output")
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def create_sample_data(self) -> pl.DataFrame:
        """Create sample data for demonstration"""
        logger.info("📊 Creating sample dataset...")
        
        # Set seed for reproducibility
        np.random.seed(42)
        
        # Generate sample e-commerce data
        n_records = 10000
        start_date = datetime(2023, 1, 1)
        
        data = {
            "order_id": range(1, n_records + 1),
            "customer_id": np.random.randint(1, 1000, n_records),
            "product_category": np.random.choice([
                "Electronics", "Clothing", "Books", "Home", "Sports", "Beauty"
            ], n_records),
            "product_name": [f"Product_{i}" for i in range(n_records)],
            "quantity": np.random.randint(1, 10, n_records),
            "unit_price": np.round(np.random.uniform(10.0, 500.0, n_records), 2),
            "order_date": [
                start_date + timedelta(days=np.random.randint(0, 365))
                for _ in range(n_records)
            ],
            "customer_age": np.random.randint(18, 80, n_records),
            "customer_location": np.random.choice([
                "New York", "California", "Texas", "Florida", "Illinois"
            ], n_records),
            "discount_percentage": np.random.uniform(0, 0.3, n_records),
        }
        
        df = pl.DataFrame(data)
        logger.info(f"✅ Created sample dataset with {len(df)} records")
        return df
    
    def basic_analysis(self, df: pl.DataFrame) -> Dict[str, pl.DataFrame]:
        """Perform basic data analysis"""
        logger.info("🔍 Performing basic analysis...")
        
        results = {}
        
        # Basic statistics
        results["basic_stats"] = df.select([
            pl.col("quantity").mean().alias("avg_quantity"),
            pl.col("unit_price").mean().alias("avg_price"),
            pl.col("customer_age").mean().alias("avg_age"),
            pl.col("order_id").count().alias("total_orders"),
        ])
        
        # Sales by category
        results["category_sales"] = (
            df.with_columns([
                (pl.col("quantity") * pl.col("unit_price") * (1 - pl.col("discount_percentage")))
                .alias("total_amount")
            ])
            .group_by("product_category")
            .agg([
                pl.col("total_amount").sum().alias("total_sales"),
                pl.col("order_id").count().alias("order_count"),
                pl.col("total_amount").mean().alias("avg_order_value")
            ])
            .sort("total_sales", descending=True)
        )
        
        # Monthly trends
        results["monthly_trends"] = (
            df.with_columns([
                pl.col("order_date").dt.strftime("%Y-%m").alias("month"),
                (pl.col("quantity") * pl.col("unit_price") * (1 - pl.col("discount_percentage")))
                .alias("total_amount")
            ])
            .group_by("month")
            .agg([
                pl.col("total_amount").sum().alias("monthly_sales"),
                pl.col("order_id").count().alias("monthly_orders")
            ])
            .sort("month")
        )
        
        # Customer analysis
        results["customer_analysis"] = (
            df.with_columns([
                (pl.col("quantity") * pl.col("unit_price") * (1 - pl.col("discount_percentage")))
                .alias("total_amount")
            ])
            .group_by("customer_id")
            .agg([
                pl.col("total_amount").sum().alias("customer_lifetime_value"),
                pl.col("order_id").count().alias("order_count"),
                pl.col("order_date").min().alias("first_order"),
                pl.col("order_date").max().alias("last_order")
            ])
            .with_columns([
                (pl.col("last_order") - pl.col("first_order")).dt.total_days().alias("customer_tenure_days")
            ])
            .sort("customer_lifetime_value", descending=True)
            .head(100)  # Top 100 customers
        )
        
        logger.info("✅ Basic analysis completed")
        return results
    
    def advanced_analysis(self, df: pl.DataFrame) -> Dict[str, pl.DataFrame]:
        """Perform advanced data analysis"""
        logger.info("🔬 Performing advanced analysis...")
        
        results = {}
        
        # Customer segmentation using RFM analysis
        today = df["order_date"].max()
        
        results["rfm_analysis"] = (
            df.with_columns([
                (pl.col("quantity") * pl.col("unit_price") * (1 - pl.col("discount_percentage")))
                .alias("total_amount")
            ])
            .group_by("customer_id")
            .agg([
                (today - pl.col("order_date").max()).dt.total_days().alias("recency"),
                pl.col("order_id").count().alias("frequency"),
                pl.col("total_amount").sum().alias("monetary")
            ])
            .with_columns([
                pl.col("recency").qcut(5, labels=["1", "2", "3", "4", "5"]).alias("r_score"),
                pl.col("frequency").qcut(5, labels=["1", "2", "3", "4", "5"]).alias("f_score"),
                pl.col("monetary").qcut(5, labels=["1", "2", "3", "4", "5"]).alias("m_score")
            ])
            .with_columns([
                (pl.col("r_score") + pl.col("f_score") + pl.col("m_score")).alias("rfm_score")
            ])
        )
        
        # Product affinity analysis
        results["product_affinity"] = (
            df.group_by(["customer_id", "product_category"])
            .agg([
                pl.col("order_id").count().alias("purchase_count")
            ])
            .pivot(values="purchase_count", index="customer_id", columns="product_category")
            .fill_null(0)
        )
        
        # Seasonal trends
        results["seasonal_trends"] = (
            df.with_columns([
                pl.col("order_date").dt.month().alias("month"),
                pl.col("order_date").dt.quarter().alias("quarter"),
                (pl.col("quantity") * pl.col("unit_price") * (1 - pl.col("discount_percentage")))
                .alias("total_amount")
            ])
            .group_by(["quarter", "month", "product_category"])
            .agg([
                pl.col("total_amount").sum().alias("sales"),
                pl.col("order_id").count().alias("orders")
            ])
            .sort(["quarter", "month", "product_category"])
        )
        
        logger.info("✅ Advanced analysis completed")
        return results
    
    def data_quality_checks(self, df: pl.DataFrame) -> Dict[str, Union[pl.DataFrame, dict]]:
        """Perform data quality checks"""
        logger.info("✅ Performing data quality checks...")
        
        results = {}
        
        # Missing values
        results["missing_values"] = df.null_count()
        
        # Duplicate records
        results["duplicates"] = {
            "total_duplicates": df.filter(df.is_duplicated()).height,
            "duplicate_orders": df.filter(df["order_id"].is_duplicated()).height
        }
        
        # Data type validation
        results["data_types"] = df.dtypes
        
        # Value ranges
        results["value_ranges"] = df.select([
            pl.col("quantity").min().alias("min_quantity"),
            pl.col("quantity").max().alias("max_quantity"),
            pl.col("unit_price").min().alias("min_price"),
            pl.col("unit_price").max().alias("max_price"),
            pl.col("customer_age").min().alias("min_age"),
            pl.col("customer_age").max().alias("max_age"),
        ])
        
        # Outliers (using IQR method)
        results["outliers"] = (
            df.select([
                pl.col("unit_price").quantile(0.25).alias("q1_price"),
                pl.col("unit_price").quantile(0.75).alias("q3_price"),
                pl.col("quantity").quantile(0.25).alias("q1_quantity"),
                pl.col("quantity").quantile(0.75).alias("q3_quantity"),
            ])
        )
        
        logger.info("✅ Data quality checks completed")
        return results
    
    def save_results(self, results: Dict[str, pl.DataFrame], format: str = "parquet"):
        """Save analysis results to files"""
        logger.info(f"💾 Saving results in {format} format...")
        
        for name, df in results.items():
            if isinstance(df, pl.DataFrame):
                if format == "parquet":
                    df.write_parquet(self.output_dir / f"{name}.parquet")
                elif format == "csv":
                    df.write_csv(self.output_dir / f"{name}.csv")
                elif format == "json":
                    df.write_json(self.output_dir / f"{name}.json")
                
                logger.info(f"📄 Saved {name} ({len(df)} records)")
    
    def create_visualizations(self, results: Dict[str, pl.DataFrame]):
        """Create basic visualizations (requires matplotlib/plotly)"""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            logger.info("📊 Creating visualizations...")
            
            # Set style
            plt.style.use('seaborn-v0_8')
            sns.set_palette("husl")
            
            # Category sales chart
            if "category_sales" in results:
                fig, ax = plt.subplots(figsize=(12, 6))
                category_data = results["category_sales"].to_pandas()
                
                sns.barplot(data=category_data, x="product_category", y="total_sales", ax=ax)
                ax.set_title("Sales by Product Category")
                ax.set_xlabel("Product Category")
                ax.set_ylabel("Total Sales ($)")
                plt.xticks(rotation=45)
                plt.tight_layout()
                plt.savefig(self.output_dir / "category_sales.png", dpi=300, bbox_inches='tight')
                plt.close()
            
            # Monthly trends
            if "monthly_trends" in results:
                fig, ax = plt.subplots(figsize=(12, 6))
                monthly_data = results["monthly_trends"].to_pandas()
                
                ax.plot(monthly_data["month"], monthly_data["monthly_sales"], marker='o')
                ax.set_title("Monthly Sales Trends")
                ax.set_xlabel("Month")
                ax.set_ylabel("Monthly Sales ($)")
                plt.xticks(rotation=45)
                plt.tight_layout()
                plt.savefig(self.output_dir / "monthly_trends.png", dpi=300, bbox_inches='tight')
                plt.close()
            
            logger.info("✅ Visualizations created")
            
        except ImportError:
            logger.warning("⚠️ Matplotlib/Seaborn not available, skipping visualizations")
    
    def run_pipeline(self) -> Dict[str, pl.DataFrame]:
        """Run the complete data processing pipeline"""
        logger.info("🚀 Starting data processing pipeline...")
        
        # Create sample data
        sample_df = self.create_sample_data()
        
        # Perform analysis
        basic_results = self.basic_analysis(sample_df)
        advanced_results = self.advanced_analysis(sample_df)
        
        # Combine results
        all_results = {**basic_results, **advanced_results}
        
        # Perform data quality checks
        quality_results = self.data_quality_checks(sample_df)
        logger.info(f"Data Quality Report: {quality_results}")
        
        # Save results
        self.save_results(all_results, format="parquet")
        self.save_results(all_results, format="csv")
        
        # Create visualizations
        self.create_visualizations(all_results)
        
        logger.info("✅ Pipeline finished successfully!")
        return all_results

if __name__ == "__main__":
    pipeline = DataPipeline()
    pipeline.run_pipeline()
