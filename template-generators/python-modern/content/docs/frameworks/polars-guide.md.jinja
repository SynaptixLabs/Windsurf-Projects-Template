# Polars Data Processing Guide

## Overview
Polars is a lightning-fast DataFrame library implemented in Rust with Python bindings. It provides 5-10x performance improvements over Pandas for many operations.

## ðŸŒŸ Key Features
- âš¡ **Blazing Fast**: 5-10x faster than Pandas
- ðŸ§  **Memory Efficient**: Optimized memory usage
- ðŸ”„ **Lazy Evaluation**: Optimized query plans
- ðŸ¦€ **Rust-powered**: Built for performance and safety
- ðŸ”§ **Easy Migration**: Similar API to Pandas
- ðŸ“Š **Rich Features**: Advanced analytics and operations

## ðŸš€ Quick Start

### 1. Basic Operations
```python
import polars as pl

# Create DataFrame
df = pl.DataFrame({
    "name": ["Alice", "Bob", "Charlie"],
    "age": [25, 30, 35],
    "city": ["NYC", "LA", "Chicago"]
})

# Basic operations
print(df.head())
print(df.describe())
print(df.filter(pl.col("age") > 25))
```

### 2. Lazy Evaluation
```python
# Lazy operations (recommended for large datasets)
lazy_df = (
    pl.scan_csv("large_file.csv")
    .filter(pl.col("age") > 25)
    .group_by("city")
    .agg(pl.col("age").mean())
    .sort("age")
)

# Execute when ready
result = lazy_df.collect()
```

### 3. Advanced Operations
```python
# Window functions
df.with_columns([
    pl.col("age").mean().over("city").alias("avg_age_by_city"),
    pl.col("age").rank().over("city").alias("age_rank")
])

# Complex aggregations
df.group_by("city").agg([
    pl.col("age").mean().alias("avg_age"),
    pl.col("age").std().alias("std_age"),
    pl.col("name").count().alias("count")
])
```

## ðŸ“š Examples in This Project

### Running the Example Pipeline
```bash
# Run the complete pipeline
python src/{{project_slug}}/data/example_pipeline.py

# Interactive mode
python src/{{project_slug}}/data/example_pipeline.py --interactive
```

### Example Features Demonstrated
- âœ… Sample data generation
- âœ… Basic statistical analysis
- âœ… Advanced analytics (RFM analysis)
- âœ… Data quality checks
- âœ… Data visualization
- âœ… Multiple export formats (Parquet, CSV, JSON)
- âœ… Interactive analysis mode

## ðŸ—ï¸ Data Processing Architecture

### Project Structure
```
src/{{project_slug}}/data/
â”œâ”€â”€ processors/          # Data processing logic
â”œâ”€â”€ loaders/            # Data loading utilities
â”œâ”€â”€ transformers/       # Data transformation functions
â”œâ”€â”€ validators/         # Data validation rules
â”œâ”€â”€ exporters/          # Data export utilities
â””â”€â”€ example_pipeline.py # Complete example
```

### Base Processor Class
```python
from abc import ABC, abstractmethod
import polars as pl

class BaseProcessor(ABC):
    @abstractmethod
    def process(self, df: pl.DataFrame) -> pl.DataFrame:
        pass
    
    def validate_input(self, df: pl.DataFrame) -> bool:
        return isinstance(df, pl.DataFrame) and not df.is_empty()
```

## ðŸ”§ Advanced Features

### 1. Lazy Evaluation
```python
# Build query plan without execution
lazy_query = (
    pl.scan_csv("data.csv")
    .filter(pl.col("date") >= "2023-01-01")
    .group_by("category")
    .agg([
        pl.col("sales").sum(),
        pl.col("profit").mean()
    ])
    .sort("sales", descending=True)
)

# Optimize and execute
result = lazy_query.collect()
```

### 2. Window Functions
```python
# Moving averages
df.with_columns([
    pl.col("sales").rolling_mean(window_size=7).alias("7_day_avg"),
    pl.col("sales").rolling_sum(window_size=30).alias("30_day_sum")
])

# Ranking within groups
df.with_columns([
    pl.col("sales").rank("dense").over("region").alias("sales_rank")
])
```

### 3. Complex Joins
```python
# Multiple join types
customers = pl.DataFrame({"id": [1, 2, 3], "name": ["A", "B", "C"]})
orders = pl.DataFrame({"customer_id": [1, 1, 2], "amount": [100, 200, 150]})

result = customers.join(
    orders, 
    left_on="id", 
    right_on="customer_id", 
    how="left"
)
```

### 4. Time Series Operations
```python
# Date parsing and manipulation
df.with_columns([
    pl.col("date").str.strptime(pl.Date, "%Y-%m-%d"),
    pl.col("date").dt.year().alias("year"),
    pl.col("date").dt.month().alias("month"),
    pl.col("date").dt.weekday().alias("weekday")
])

# Time-based grouping
df.group_by_dynamic("date", every="1mo").agg([
    pl.col("sales").sum(),
    pl.col("orders").count()
])
```

## ðŸ“Š Performance Optimization

### 1. Memory Usage
```python
# Use lazy evaluation for large datasets
lazy_df = pl.scan_csv("huge_file.csv")

# Stream processing for larger-than-memory datasets
for batch in pl.scan_csv("huge_file.csv").collect().iter_slices(1000):
    process_batch(batch)

# Optimize data types
df.with_columns([
    pl.col("category").cast(pl.Categorical),
    pl.col("amount").cast(pl.Float32)  # If precision allows
])
```

### 2. Query Optimization
```python
# Predicate pushdown - filter early
pl.scan_csv("data.csv").filter(pl.col("date") > "2023-01-01").collect()

# Column selection - only load needed columns
pl.scan_csv("data.csv").select(["date", "sales", "region"]).collect()

# Use efficient joins
large_df.join(small_df.lazy(), on="key", how="inner").collect()
```

### 3. Parallel Processing
```python
# Polars automatically uses all CPU cores
# Control with environment variable
import os
os.environ["POLARS_MAX_THREADS"] = "4"

# Or programmatically
pl.Config.set_global_table_parallel_num_threads(4)
```

## ðŸ”„ Migration from Pandas

### Common Operations
```python
# Pandas vs Polars syntax comparison

# Pandas
df.groupby('category').agg({'sales': 'sum', 'profit': 'mean'})

# Polars
df.group_by('category').agg([
    pl.col('sales').sum(),
    pl.col('profit').mean()
])

# Pandas
df[df['age'] > 25]

# Polars
df.filter(pl.col('age') > 25)

# Pandas
df.assign(total=df['quantity'] * df['price'])

# Polars
df.with_columns((pl.col('quantity') * pl.col('price')).alias('total'))
```

### Key Differences
1. **Immutable**: Polars DataFrames are immutable by default
2. **Explicit**: Operations are more explicit (e.g., `pl.col()`)
3. **Lazy**: Encourages lazy evaluation for optimization
4. **Strict**: Stricter about data types and operations

## ðŸ§ª Testing with Polars

### Unit Testing
```python
import polars as pl
import pytest

def test_data_transformation():
    # Sample data
    df = pl.DataFrame({
        "value": [1, 2, 3, 4, 5],
        "category": ["A", "A", "B", "B", "C"]
    })
    
    # Transform
    result = df.group_by("category").agg(pl.col("value").sum())
    
    # Assert
    expected = pl.DataFrame({
        "category": ["A", "B", "C"],
        "value": [3, 7, 5]
    })
    
    assert result.sort("category").equals(expected.sort("category"))
```

### Property-Based Testing
```python
from hypothesis import given, strategies as st
import polars as pl

@given(st.lists(st.integers(), min_size=1))
def test_sum_property(values):
    df = pl.DataFrame({"values": values})
    result = df.select(pl.col("values").sum()).item()
    assert result == sum(values)
```

## ðŸ­ Production Deployment

### 1. Memory Management
```python
# Monitor memory usage
import psutil

def process_with_memory_monitoring(df):
    initial_memory = psutil.virtual_memory().used
    
    result = df.lazy().select([
        pl.col("*"),
        pl.col("sales").sum().over("region").alias("region_total")
    ]).collect()
    
    final_memory = psutil.virtual_memory().used
    logger.info(f"Memory used: {(final_memory - initial_memory) / 1024**2:.2f} MB")
    
    return result
```

### 2. Error Handling
```python
def safe_data_processing(file_path: str) -> pl.DataFrame:
    try:
        return pl.scan_csv(file_path).collect()
    except pl.ComputeError as e:
        logger.error(f"Polars compute error: {e}")
        raise
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        raise
```

### 3. Configuration Management
```python
# polars_config.py
class PolarsConfig:
    MAX_THREADS = 8
    MEMORY_LIMIT = "8GB"
    LAZY_EVALUATION = True
    
    @classmethod
    def configure(cls):
        pl.Config.set_global_table_parallel_num_threads(cls.MAX_THREADS)
        if cls.MEMORY_LIMIT:
            os.environ["POLARS_MAX_MEMORY"] = cls.MEMORY_LIMIT
```

## ðŸ“Š Integration with Other Tools

### 1. Database Integration
```python
# With ConnectorX (fastest)
import connectorx as cx
df = pl.from_pandas(cx.read_sql("SELECT * FROM table", "postgresql://..."))

# With SQLAlchemy
from sqlalchemy import create_engine
engine = create_engine("postgresql://...")
df = pl.read_database("SELECT * FROM table", engine)
```

### 2. Apache Arrow Integration
```python
# Convert to/from Arrow
arrow_table = df.to_arrow()
df_from_arrow = pl.from_arrow(arrow_table)

# Direct Arrow operations
import pyarrow.compute as pc
arrow_result = pc.sum(df.to_arrow()["column"])
```

### 3. Visualization
```python
# With Plotly
import plotly.express as px

# Convert to Pandas for plotting
pandas_df = df.to_pandas()
fig = px.scatter(pandas_df, x="x", y="y", color="category")

# Or use Polars directly with Altair
import altair as alt
alt.Chart(df.to_pandas()).mark_circle().encode(x="x", y="y")
```

## ðŸ” Debugging and Profiling

### 1. Query Plans
```python
# Show query plan
lazy_query = df.lazy().filter(pl.col("age") > 25).group_by("city").agg(pl.col("age").mean())
print(lazy_query.explain())
```

### 2. Performance Profiling
```python
import time

def profile_operation(df, operation_name):
    start = time.time()
    result = df.lazy().select([
        pl.col("*"),
        pl.col("value").rolling_mean(100).alias("rolling_mean")
    ]).collect()
    end = time.time()
    logger.info(f"{operation_name} took {end - start:.2f} seconds")
    return result
```
