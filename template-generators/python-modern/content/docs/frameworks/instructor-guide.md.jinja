# Instructor Integration Guide

## Overview
Instructor is a Python library that makes it a breeze to work with structured outputs from large language models (LLMs). Built on top of Pydantic, it provides a simple, transparent, and user-friendly API to manage validation, retries, and streaming responses.

## ðŸŒŸ Key Features
- ðŸ›¡ï¸ **Type Safety**: Automatic validation using Pydantic models
- ðŸ”„ **Automatic Retries**: Built-in retry logic for failed validations
- ðŸ“Š **Streaming Support**: Stream structured outputs in real-time
- ðŸŽ¯ **Multiple Providers**: OpenAI, Anthropic, Cohere, and more
- ðŸ”§ **Easy Integration**: Drop-in replacement for LLM client calls

## ðŸš€ Quick Start

### 1. Environment Setup
Create a `.env` file in your project root:
```bash
# Choose your LLM provider
OPENAI_API_KEY=your_openai_key_here
# ANTHROPIC_API_KEY=your_anthropic_key_here
```

### 2. Basic Usage
```python
import instructor
import openai
from pydantic import BaseModel

# Patch the OpenAI client
client = instructor.from_openai(openai.OpenAI())

class UserInfo(BaseModel):
    name: str
    age: int
    email: str

# Extract structured data
user = client.chat.completions.create(
    model="gpt-4o-mini",
    response_model=UserInfo,
    messages=[{"role": "user", "content": "John is 25 years old and his email is john@example.com"}]
)

print(user.name)  # "John"
print(user.age)   # 25
print(user.email) # "john@example.com"
```

### 3. Advanced Validation
```python
from pydantic import BaseModel, Field, validator
from typing import List

class Product(BaseModel):
    name: str = Field(description="Product name")
    price: float = Field(gt=0, description="Price must be positive")
    categories: List[str] = Field(description="Product categories")
    
    @validator('categories')
    def validate_categories(cls, v):
        if len(v) == 0:
            raise ValueError("At least one category required")
        return v

# Extract with validation
products = client.chat.completions.create(
    model="gpt-4o",
    response_model=List[Product],
    messages=[{"role": "user", "content": "Extract products from: iPhone 15 $999 in Electronics, Tech"}]
)
```

## ðŸ“š Examples in This Project

### Running Examples
```bash
# Basic extraction
python examples/instructor/basic_extraction.py

# Data analysis
python examples/instructor/data_analysis.py

# Content moderation
python examples/instructor/content_moderation.py

# CLI tool
python examples/instructor/cli_tool.py --help
```

### Available Examples
1. **Basic Extraction** - Simple data extraction patterns
2. **Data Analysis** - Structured analysis of text data
3. **Content Moderation** - Automated content classification
4. **CLI Tool** - Command-line interface using Typer

## ðŸ—ï¸ Best Practices

### 1. Use Descriptive Field Descriptions
```python
class Task(BaseModel):
    title: str = Field(description="Clear, actionable task title")
    priority: int = Field(ge=1, le=5, description="Priority from 1 (low) to 5 (high)")
    due_date: Optional[datetime] = Field(description="Due date in ISO format")
```

### 2. Implement Custom Validators
```python
from pydantic import validator
import re

class Email(BaseModel):
    address: str
    
    @validator('address')
    def validate_email(cls, v):
        if not re.match(r'^[^@]+@[^@]+\.[^@]+$', v):
            raise ValueError('Invalid email format')
        return v
```

### 3. Handle Streaming Responses
```python
from instructor import Partial

# Stream partial responses
for partial_user in client.chat.completions.create_partial(
    model="gpt-4o",
    response_model=UserInfo,
    messages=[{"role": "user", "content": "Extract user info..."}]
):
    print(partial_user)  # Partial updates as they arrive
```

### 4. Use Retry Logic
```python
import instructor
from tenacity import Retrying, stop_after_attempt, wait_fixed

client = instructor.from_openai(
    openai.OpenAI(),
    mode=instructor.Mode.TOOLS,
    max_retries=Retrying(
        stop=stop_after_attempt(3),
        wait=wait_fixed(1),
    ),
)
```

## ðŸ”§ Configuration Options

### Response Modes
```python
# Function calling (default, most reliable)
client = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)

# JSON mode (faster, less reliable)
client = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.JSON)

# Markdown mode (for models without function calling)
client = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.MD_JSON)
```

### Multiple Providers
```python
# OpenAI
import openai
client = instructor.from_openai(openai.OpenAI())

# Anthropic
import anthropic
client = instructor.from_anthropic(anthropic.Anthropic())

# Cohere
import cohere
client = instructor.from_cohere(cohere.Client())

# Groq
import groq
client = instructor.from_groq(groq.Groq())
```

## ðŸ­ Production Patterns

### 1. Error Handling
```python
from pydantic import ValidationError
import instructor

try:
    result = client.chat.completions.create(
        model="gpt-4o",
        response_model=UserInfo,
        messages=[{"role": "user", "content": query}]
    )
except ValidationError as e:
    print(f"Validation failed: {e}")
    # Handle validation errors
except instructor.InstructorRetryException as e:
    print(f"Max retries exceeded: {e}")
    # Handle retry failures
```

### 2. Batch Processing
```python
import asyncio
from typing import List

async def process_batch(queries: List[str]) -> List[UserInfo]:
    tasks = []
    for query in queries:
        task = client.chat.completions.create(
            model="gpt-4o",
            response_model=UserInfo,
            messages=[{"role": "user", "content": query}]
        )
        tasks.append(task)
    
    return await asyncio.gather(*tasks)
```

### 3. Caching Results
```python
import functools
import hashlib

@functools.lru_cache(maxsize=1000)
def cached_extraction(query_hash: str, model: str):
    # Implement your caching logic
    pass

def extract_with_cache(query: str, model: str = "gpt-4o"):
    query_hash = hashlib.md5(query.encode()).hexdigest()
    return cached_extraction(query_hash, model)
```

### 4. Monitoring and Logging
```python
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

def log_extraction(query: str, result: BaseModel, duration: float):
    logger.info(
        f"Extraction completed",
        extra={
            "query_length": len(query),
            "result_type": type(result).__name__,
            "duration_ms": duration * 1000,
            "timestamp": datetime.now().isoformat()
        }
    )
```

## ðŸ§ª Testing Your Extractors

### Unit Testing
```python
import pytest
from unittest.mock import Mock, patch

@pytest.fixture
def mock_client():
    client = Mock()
    client.chat.completions.create.return_value = UserInfo(
        name="Test User",
        age=30,
        email="test@example.com"
    )
    return client

def test_user_extraction(mock_client):
    result = extract_user_info("John is 25", client=mock_client)
    assert result.name == "Test User"
    assert result.age == 30
```

### Integration Testing
```python
@pytest.mark.integration
def test_real_extraction():
    """Test with real API (requires API key)"""
    if not os.getenv('OPENAI_API_KEY'):
        pytest.skip("No API key available")
    
    result = extract_user_info("Alice is 28 years old, email: alice@test.com")
    assert result.name.lower() == "alice"
    assert result.age == 28
    assert "alice@test.com" in result.email
```
